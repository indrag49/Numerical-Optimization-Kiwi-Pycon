{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "\n",
    "+ **Jorge Nocedal, Stephen J. Wright**. *Numerical Optimization*\n",
    "+ **Jan A. Snyman, Danile N. Wilke**. *Practical Mathematical Optimization*\n",
    "+ **Singiresu S. Rao**. *Engineering Optimization: Theory and Practice*\n",
    "+ **James F. Epperson**. An Introduction to Numerical Methods and Analysis*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Optimization\n",
    "\n",
    "+ Let $f(\\mathbf{x})$ be a scalar function of a vector of variables $\\mathbf{x} = \\begin{pmatrix}x_1 \\\\ x_2 \\\\ \\vdots \\\\ x_n \\end{pmatrix} \\in \\mathbb{R}^n$. *Numerical Optimization* is the minimization or maximization of this function $f$ subject to constraints on $\\mathbf{x}$. This $f$ is a scalar function of $\\mathbf{x}$, also known as the *objective function* and the continuous components $x_i \\in \\mathbf{x}$ are called the *decision variables*.\n",
    "\n",
    "+ The optimization problem is formulated in the following way:\n",
    "\n",
    "\n",
    "\\begin{align}\n",
    "&\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n}        &\\qquad& f(\\mathbf{x}) \\\\\n",
    "&\\text{subject to} &      & g_k(\\mathbf{x}) \\leq 0,\\ k=1,2,..., m\\\\\n",
    "&                  &      & h_k(\\mathbf{x}) = 0,\\ k=1,2,..., r\\\\\n",
    "&                  &      & m,r < n. \\label{eq:1}\\tag{1}\n",
    "\\end{align}\n",
    "\n",
    "+ Here, $g_k(\\mathbf{x})$ and $h_k(\\mathbf{x})$ are scalar functions too (like $f(\\mathbf{x})$) and are called *constraint functions*. The constraint functions define some specific equations and/or inequalities that $\\mathbf{x}$ should satisfy.\n",
    "\n",
    "\n",
    "## A Solution\n",
    "  \n",
    "A *solution* of $f(\\mathbf{x})$ is a point $\\mathbf{x^*}$ which denotes the optimum vector that solves Eq.\\eqref{eq:1}, corresponding to the optimum value $f(\\mathbf{x^*})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case of a *minimization* problem, the optimum vector $\\mathbf{x^*}$ is referred to as the *global minimizer* of $f$, and $f$ attains the least possible value at $\\mathbf{x^*}$. To design an algorithm that finds out the global minimizer for a function is quite difficult, as in most cases we do not have the idea of the overall shape of $f$. Mostly our knowledge is restricted to a local portion of $f$.\n",
    "\n",
    "+ A point $\\mathbf{x^*}$ is called a *global minimizer* of $f$ if $f(\\mathbf{x^*}) \\leq f(\\mathbf{x}) \\forall\\ x$.\n",
    "\n",
    " \n",
    "+ A point $\\mathbf{x^*}$ is called a *local minimizer* of $f$ if there is a neighborhood $\\mathcal{N}$ of $\\mathbf{x^*}$ such that $f(\\mathbf{x^*}) \\leq f(\\mathbf{x}) \\forall\\ x \\in \\mathcal{N}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximization\n",
    "+ We just defined a *minimization* problem as our optimization task. We could do the same with a *maximization* problem with little tweaks. The problem $\\underset{\\mathbf{x} \\in \\mathbb{R}^n}{max} f(\\mathbf{x})$ can be formulated as:\n",
    "\\begin{equation}\n",
    "\\label{eq:2}\\tag{2}\n",
    "    \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{max} f(\\mathbf{x}) = - \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{min}\\{- f(\\mathbf{x})\\}\n",
    "\\end{equation}\n",
    "\n",
    "+ We then apply any minimization technique after setting $\\hat{f}(\\mathbf{x}) = - f(\\mathbf{x})$. Further, for the inequality constraints for the maximization problem, given by $g_k(\\mathbf{x}) \\geq 0$, we set \n",
    "\\begin{equation}\n",
    "\\label{eq:3}\\tag{3}\n",
    "    \\hat{g}_k(\\mathbf{x})=-g_k(\\mathbf{x})\n",
    "    (\\#eq:3)\n",
    "\\end{equation}\n",
    "\n",
    "+ The problem thus has become,\n",
    "\n",
    "\\begin{align}\n",
    "&\\!\\min_{\\mathbf{x} \\in \\mathbb{R}^n}        &\\qquad& \\hat{f}(\\mathbf{x})\\\\ \n",
    "&\\text{subject to} &      & \\hat{g}_k(\\mathbf{x}) \\leq 0,\\ k=1,2,..., m\\\\\n",
    "&                  &      & h_k(\\mathbf{x}) = 0,\\ k=1,2,..., r\\\\\n",
    "&                  &      & m,r < n.\\label{eq:4}\\tag{4}\n",
    "\\end{align}\n",
    "\n",
    "+ After the solution $\\mathbf{x^*}$ is computed, the maximum value of the problem is given by: $-\\hat{f}(\\mathbf{x^*})$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling of Decision Variables\n",
    "\n",
    "While formulating optimization problems, it must be guaranteed that the scale of the decision variables are approximately of the same order. If this is not taken care of, some optimization algorithms that are sensitive to scaling will perform poorly and will flounder to converge to the solution. Two of the fundamental fields that get disturbed due to poor scaling are computing the optimized step lengths and the numerical gradients. One of the widely accepted best practices is to make the decision variables dimensionless and vary them approximately between 0 and 1. One should always prefer optimization algorithms that are not sensitive to scaling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Vector and Hessian Matrix of the Objective Function\n",
    "\n",
    "+ For a differentiable objective function $f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}$, its *gradient vector* given by $\\nabla f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$, is defined at the point $\\mathbf{x}$ in the $n$-dimensional space as the vector of first order partial derivatives:\n",
    "\\begin{equation}\n",
    "\\nabla f(\\mathbf{x})= \\begin{pmatrix} \\frac{\\partial f}{\\partial x_1}(\\mathbf{x})\\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\partial f}{\\partial x_n}(\\mathbf{x})\n",
    "    \\end{pmatrix} \\label{eq:5}\\tag{5}\n",
    "\\end{equation}\n",
    "\n",
    "+ For a twice continuously differentiable function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, its *Hessian matrix* given by $\\mathbf{H}(f(\\mathbf{x}))$ is defined at the point $\\mathbf{x}$ in the $n \\times n$-dimensional space as the matrix of second order partial derivatives:\n",
    "\\begin{equation}\n",
    "    \\mathbf{H} f(\\mathbf{x})=\\frac{\\partial ^2 f}{\\partial x_i \\partial x_j} = \\begin{pmatrix} \\frac{\\partial^2 f}{\\partial x_1^2}(\\mathbf{x}) & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2}(\\mathbf{x}) & \\ldots & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_n}(\\mathbf{x})\\\\\n",
    "    \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1}(\\mathbf{x}) & \\frac{\\partial^2 f}{\\partial x_2^2}(\\mathbf{x}) & \\ldots & \\frac{\\partial^2 f}{\\partial x_2 \\partial x_n}(\\mathbf{x}) \\\\\n",
    "    \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "    \\frac{\\partial^2 f}{\\partial x_n \\partial x_1}(\\mathbf{x}) & \\frac{\\partial^2 f}{\\partial x_n \\partial x_2}(\\mathbf{x}) & \\ldots & \\frac{\\partial^2 f}{\\partial x_n^2}(\\mathbf{x})\n",
    "    \\end{pmatrix}\\label{eq:6}\\tag{6}\n",
    "\\end{equation}\n",
    "\n",
    "+ One important relation that we will keep in mind is that the *Hessian matrix* is the *Jacobian* of the *gradient vector* of $f(\\mathbf{x})$, where the *Jacobian matrix* of a vector-valued function $\\mathbf{F}(\\mathbf{x})$ is the matrix of all its first order partial derivatives, given by, $\\mathbf{JF}(\\mathbf{x})= \\begin{pmatrix} \\frac{\\partial \\mathbf{F}}{\\partial x_1} & \\ldots \\frac{\\partial \\mathbf{F}}{\\partial x_n} \\end{pmatrix}$. The relation is as followed:\n",
    "\\begin{equation}\n",
    "    \\mathbf{H} f(\\mathbf{x}) = \\mathbf{J}(\\nabla f(\\mathbf{x})) \\label{eq:7}\\tag{7}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example\n",
    "\n",
    "Let an objective function be $f(\\mathbf{x}) = 2x_1x_2^3+3x_2^2x_3 + x_3^3x_1$. We will find out the gradient vector $\\nabla f(\\mathbf{x})$ and the Hessian matrix $\\mathbf{H} f(\\mathbf{x})$ at the point $\\mathbf{p} = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}$. The gradient vector is $\\nabla f(\\mathbf{x}) = \\begin{pmatrix} 2x_2^3+x_3^3 \\\\ 6x_1x_2^2+6x_2x_3 \\\\ 3x_2^2+3x_3^2x_1 \\end{pmatrix}$. So $\\nabla f(\\mathbf{x})| \\mathbf{p} = \\begin{pmatrix} 43 \\\\ 60 \\\\ 39 \\end{pmatrix}$. The Hessian matrix is therefore given by, $\\mathbf{H}f(\\mathbf{x}) = \\begin{pmatrix} 0 & 6x_2^2 & 3x_3^2 \\\\ 6x_2^2 & 12x_1x_2+6x_3 & 6x_2 \\\\ 3x_3^2 & 6x_2 & 6x_3x_1 \\end{pmatrix}$ and at point $\\mathbf{p}$, $\\mathbf{H} f(\\mathbf{x})|\\mathbf{p} = \\begin{pmatrix} 0 & 24 & 27 \\\\ 24 & 42 & 12 \\\\ 27 & 12 & 18 \\end{pmatrix}$.\n",
    "\n",
    "We will try to work out the same example with Python scripting now. For that we need an extra package called [`autograd`](https://github.com/HIPS/autograd), besides the [`numpy`](https://numpy.org/) package. The `autograd` package is used for automatically differentiating native Python and Numpy code. Fundamentally `autograd` is used in *gradient-based optimization*. First `pip` install the `autograd package`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: autograd in c:\\users\\indra\\anaconda3\\lib\\site-packages (1.3)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: future>=0.15.2 in c:\\users\\indra\\anaconda3\\lib\\site-packages (from autograd) (0.18.2)\n",
      "Requirement already satisfied: numpy>=1.12 in c:\\users\\indra\\anaconda3\\lib\\site-packages (from autograd) (1.20.3)\n"
     ]
    }
   ],
   "source": [
    "pip install autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient vector: [43. 60. 39.]\n",
      "Hessian matrix:\n",
      " [[ 0. 24. 27.]\n",
      " [24. 42. 12.]\n",
      " [27. 12. 18.]]\n"
     ]
    }
   ],
   "source": [
    "import autograd.numpy as au\n",
    "from autograd import grad, jacobian \n",
    "p = np.array([1, 2, 3], dtype=float)\n",
    "def f(x): # Objective function\n",
    "    return 2*x[0]*x[1]**3+3*x[1]**2*x[2]+x[2]**3*x[0]\n",
    "grad_f = grad(f) # gradient of the objective function\n",
    "hessian_f = jacobian(grad_f) # Hessian of the objective function\n",
    "print(\"gradient vector:\",grad_f(p))\n",
    "print(\"Hessian matrix:\\n\",hessian_f(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directional Derivative of the Objective Function\n",
    "\n",
    "For a real valued objective function $f(\\mathbf{x})$ and a feasible direction $\\mathbf{\\delta}$, the *directional derivative* of $f(\\mathbf{x})$ in the direction $\\mathbf{\\delta}$ is given by:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial f}{\\partial \\mathbf{\\delta}}(\\mathbf{x}) = \\lim_{\\alpha \\to 0} \\frac{f(\\mathbf{x} + \\alpha \\mathbf{\\delta}) - f(\\mathbf{x})}{\\alpha} \\label{eq:8}\\tag{8}\n",
    "\\end{equation}\n",
    "where $\\|\\mathbf{\\delta}\\| = 1$.\n",
    "\n",
    "Now for $\\mathbf{x} \\in \\mathbb{R}^n$, let us consider the differential equation:\n",
    "\\begin{equation}\n",
    "    df(\\mathbf{x}) = \\frac{\\partial f(\\mathbf{x})}{\\partial x_1}dx_1 + \\ldots + \\frac{\\partial f(\\mathbf{x})}{\\partial x_n}dx_n = \\nabla^Tf(\\mathbf{x})d\\mathbf{x} = \\langle \\nabla f(\\mathbf{x}), d\\mathbf{x} \\rangle \\label{eq:9}\\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "where $\\langle .,. \\rangle$ denotes the dot product between two matrices and/or vectors. Now let us consider a function $\\hat{f}(\\mathbf{x}) = f(\\hat{\\mathbf{x}} + \\alpha \\mathbf{\\delta})$, such that for a point $\\mathbf{x}$ passing through the point $\\hat{\\mathbf{x}}$ on the line through $\\hat{\\mathbf{x}}$ in the direction $\\mathbf{\\delta}$ is given by $\\mathbf{x}(\\alpha) = \\hat{\\mathbf{x}} + \\alpha \\mathbf{\\delta}$. now, for an infinitesimal change $d\\alpha$, we have $d\\mathbf{x}=\\mathbf{\\delta}d\\alpha$. Thus, the differential at the point $\\mathbf{x}$ in the given direction is $d\\hat{f}=\\nabla^Tf(\\mathbf{x})\\delta d\\alpha$ So, the directional derivative now can be written as:\n",
    "\\begin{equation}\n",
    "    \\frac{\\partial f}{\\partial \\mathbf{\\delta}}(\\mathbf{x}) = \\frac{d}{d\\alpha}f(\\mathbf{x}+\\alpha\\mathbf{\\delta})|_{\\alpha=0} = \\nabla^Tf(\\mathbf{x})\\mathbf{\\delta} \\label{eq:10}\\tag{10}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Example\n",
    "\n",
    "Let an objective function be $f(\\mathbf{x}) = 2x_1x_2^3+3x_2^2x_3 + x_3^3x_1$. We will find out the gradient vector $\\nabla f(\\mathbf{x})$ at the point $\\mathbf{p} = \\begin{pmatrix} 1 & 2 & 3 \\end{pmatrix}$ and then calculate the directional derivative in the direction $\\mathbf{\\delta}=\\begin{pmatrix} \\frac{1}{\\sqrt{35}} & \\frac{3}{\\sqrt{35}} & \\frac{5}{\\sqrt{35}} \\end{pmatrix}$. We will use the same `autograd` package to calculate the same using Python.\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "directional derivative: 70.65489569530399\n"
     ]
    }
   ],
   "source": [
    "p = np.array([1, 2, 3], dtype=float)\n",
    "delta = np.array([1, 3, 5], dtype=float)/np.sqrt(35)\n",
    "def f(x):\n",
    "    return 2*x[0]*x[1]**3+3*x[1]**2*x[2]+x[2]**3*x[0]\n",
    "grad_f = grad(f)\n",
    "print(\"directional derivative:\", grad_f(p).dot(delta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Positive Definite and Positive Semi-definite Matrices\n",
    "\n",
    "+ A real matrix $\\mathbf{M}\\in \\mathbb{R}^{N\\times N}$ is a *positive definite* matrix if for any real vector $\\mathbf{v} \\in \\mathbb{R}^n$ other than the null vector, the following is satisfied:\n",
    "\\begin{equation}\n",
    "    \\mathbf{v}^T\\mathbf{M}\\mathbf{v} > 0 \\label{eq:11}\\tag{11}\n",
    "\\end{equation}\n",
    "\n",
    "+ A real matrix $\\mathbf{M}\\in \\mathbb{R}^{N\\times N}$ is a *positive semi-definite* matrix if for any real vector $\\mathbf{v} \\in \\mathbb{R}^n$, the following is satisfied:\n",
    "\\begin{equation}\n",
    "    \\mathbf{v}^T\\mathbf{M}\\mathbf{v} \\geq 0 \\label{eq:12}\\tag{1}\n",
    "\\end{equation}\n",
    "\n",
    "\n",
    "+ All the eigenvalues of a positive definite matrix are positive.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An Example\n",
    "\n",
    "We use a Python script to compute the eigenvalues and check whether the following matrices are positive definite, positive semi-definite or negative-definite:\n",
    "    \n",
    "* $\\begin{pmatrix}2 & -1 & 0 \\\\ -1 & 2 & -1\\\\ 0 & -1 & 2 \\end{pmatrix}$\n",
    "* $\\begin{pmatrix} -2 & 4\\\\ 4 & -8 \\end{pmatrix}$\n",
    "* $\\begin{pmatrix} -2 & 2\\\\ 2 & -4 \\end{pmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The eigenvalues of M: [3.41421356 2.         0.58578644]\n",
      "M is positive definite\n"
     ]
    }
   ],
   "source": [
    "M = np.array(([2, -1, 0], [-1, 2, -1], [0, -1, 2]), dtype=float)\n",
    "#M = np.array(([-2, 4], [4, -8]), dtype=float)\n",
    "#M = np.array(([-2, 2], [2, -4]), dtype=float)\n",
    "eigs = np.linalg.eigvals(M)\n",
    "print(\"The eigenvalues of M:\", eigs)\n",
    "if (np.all(eigs>0)):\n",
    "    print(\"M is positive definite\")\n",
    "elif (np.all(eigs>=0)):\n",
    "    print(\"M is positive semi-definite\")\n",
    "else:\n",
    "    print(\"M is negative definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Convexity?\n",
    "\n",
    "A set $\\mathbf{X} \\subset \\mathbb{R}^n$ is said to be a *convex set* if $\\forall\\ \\mathbf{x}, \\mathbf{y} \\in \\mathbf{X}$ and $\\alpha \\in [0, 1]$, the following is satisfied:\n",
    "\\begin{equation}\n",
    "    (1-\\alpha)\\mathbf{x} + \\alpha \\mathbf{y} \\in \\mathbf{X} \\label{eq:13}\\tag{13}\n",
    "\\end{equation}\n",
    "If the above condition is not satisfied, the set is a *non-convex set*.\n",
    "\n",
    "A function $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$ is called a *convex function* if for every two points $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^n$ and $\\alpha \\in [0,1]$, the following condition is satisfied:\n",
    "\\begin{equation}\n",
    "f(\\alpha \\mathbf{x} + (1-\\alpha)\\mathbf{y}) \\leq \\alpha f(\\mathbf{x})+(1-\\alpha)f(\\mathbf{y}) \\label{eq:14}\\tag{14}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Optimization Algorithms\n",
    "\n",
    "Optimization Algorithms are iterative techniques that follow the following fundamental steps:\n",
    "\n",
    "* Initialize with a guess of the decision variables $\\mathbf{x}$,\n",
    "* Iterate through the process of generating a list of improving estimates, \n",
    "* check whether the terminating conditions are met, and the estimates will be probably stop at the solution point $\\mathbf{x}^*$.\n",
    "\n",
    "The book by Nocedal and Wright [*Nocedal, Jorge, and Stephen Wright. Numerical optimization. Springer Science & Business Media, 2006.*] states that most of the optimization strategies make use of either the objective function $f(\\mathbf{x})$, the constraint functions $g(\\mathbf{x})$ and $h(\\mathbf{x})$, the first or second derivatives of these said functions, information collected during previous iterations and/or local information gathered at the present point. As Nocedal and Wright mentions, a good optimization algorithm should have the following fundamental properties:\n",
    "\n",
    "* **Robustness**: For all acceptable initial points chosen, the algorithm should operate well on a broad range of problems, in their particular class.\n",
    "* **Efficiency**: The time complexity and the space complexity of the algorithm should be practicable\n",
    "* **Accuracy**: The solution should be as precise as possible, with the caveat that it should not be too much delicate to errors in the data or to numerical rounding and/or truncating errors while it is being executed on a machine.\n",
    "\n",
    "There might be some trade offs allowed between speed and memory, between speed and robustness, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
