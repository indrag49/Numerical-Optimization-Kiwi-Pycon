{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Unconstrained Optimization Problem\n",
    "\n",
    "An unconstrained optimization problem deals with finding the local minimizer $\\mathbf{x}^*$ of a real valued and smooth objective function $f(\\mathbf{x})$ of $n$ variables, given by $f: \\mathbb{R}^n \\rightarrow \\mathbb{R}$, formulated as,\n",
    "\n",
    "\\begin{equation}\n",
    "    \\underset{\\mathbf{x} \\in \\mathbb{R}^n}{\\min f(\\mathbf{x})} \\label{eq:1}\\tag{1}\n",
    "\\end{equation}\n",
    "with no restrictions on the decision variables $\\mathbf{x}$. We work towards computing $\\mathbf{x}^*$, such that $\\forall\\ \\mathbf{x}$ near $\\mathbf{x}^*$, the following inequality is satisfied:\n",
    "\\begin{equation}\n",
    "    f(\\mathbf{x}^*) \\leq f(\\mathbf{x}) \\label{eq:2}\\tag{2}\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Necessary and Sufficient Conditions for Local Minimizer in Unconstrained Optimization\n",
    "\n",
    "### First-Order Necessary Condition\n",
    "\n",
    "If there exists a local minimizer $\\mathbf{x}^*$ for a real-valued smooth function $f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}$, in an open neighborhood $\\subset \\mathbb{R}^n$ of $\\mathbf{x}^*$ along the direction $\\mathbf{\\delta}$, then the *first order necessary condition* for the minimizer is given by:\n",
    "\\begin{equation}\n",
    "    \\nabla^Tf(\\mathbf{x}^*)\\mathbf{\\delta}=0\\ \\forall\\ \\mathbf{\\delta} \\neq 0 \\label{eq:3}\\tag{3}\n",
    "\\end{equation}\n",
    "i.e, the *directional derivative* is $0$, which ultimately reduces to the equation:\n",
    "\\begin{equation}\n",
    "    \\nabla f(\\mathbf{x}^*)=0 \\label{eq:4}\\tag{4}\n",
    "\\end{equation}\n",
    "\n",
    "### An Example\n",
    "\n",
    "The **Rosenbrock function** of $n$-variables is given by:\n",
    "\\begin{equation}\n",
    "    f(\\mathbf{x}) = \\sum_{i=1}^{n-1}(100(x_{i+1}-x_i^2)^2 + (1-x_i)^2) \\label{eq:5}\\tag{5}\n",
    "\\end{equation}\n",
    "where, $\\mathbf{x} \\in \\mathbb{R}^n$. For this example let us consider the *Rosenbrock function* for two variables, given by:\n",
    "\\begin{equation}\n",
    "    f(\\mathbf{x}) = 100(x_2-x_1^2)^2+(1-x_1)^2 \\label{eq:6}\\tag{6}\n",
    "\\end{equation}\n",
    "We will show that the first order necessary condition is satisfied for the local minimizer $\\mathbf{x^*}=\\begin{bmatrix} 1 \\\\ 1 \\end{bmatrix}$. We first check whether $\\mathbf{x}^*$ is a minimizer or not. Putting $x_1=x_2=1$ in $f(\\mathbf{x})$, we get $f(\\mathbf{x})=0$. Now, we check whether the  $\\mathbf{x^*}$ satisfies the first order necessary condition. For that we calculate $\\nabla f(\\mathbf{x}^*)$. \n",
    "\\begin{equation}\n",
    "    \\nabla f(\\mathbf{x}^*) = \\begin{bmatrix} -400x_1(x_2-x_1)^2-2(1-x_1) \\\\ 200(x_2-x_1^2)\\end{bmatrix}_{\\mathbf{x}^*} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \\label{eq:7}\\tag{7}\n",
    "\\end{equation}\n",
    "\n",
    "So, we see that the first order necessary condition is satisfied. We can do similar analysis using the `scipy.optimize` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "# Import the Rosenbrock function, its gradient and Hessian respectively\n",
    "from scipy.optimize import rosen, rosen_der, rosen_hess\n",
    "x_m = np.array([1, 1]) #given local minimizer\n",
    "rosen(x_m) # check whether x_m is a minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result is $0.0$. So $\\mathbf{x}^*$ is a minimizer. We then check for the first order necessary condition, using the gradient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rosen_der(x_m) # calculates the gradient at the point x_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This matches with our calculations and also satisfies the first-order necessary condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Second-Order Necessary Conditions\n",
    "\n",
    "If there exists a local minimizer $\\mathbf{x}^*$ for a real-valued smooth function $f(\\mathbf{x}): \\mathbb{R}^n \\rightarrow \\mathbb{R}$, in an open neighborhood $\\subset \\mathbb{R}^n$ of $\\mathbf{x}^*$ along the feasible direction $\\mathbf{\\delta}$, and $\\mathbf{H} f(\\mathbf{x})$ exists and is continuous in the open neighborhood, then the second order necessary conditions for the minimizer are given by:\n",
    "\\begin{equation}\n",
    "    \\nabla^T f(\\mathbf{x}^*)\\mathbf{\\delta} = 0, \\forall\\ \\mathbf{\\delta} \\neq 0 \\label{eq:8}\\tag{8}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\delta}^T\\mathbf{H}f(\\mathbf{x}^*)\\mathbf{\\delta} \\geq 0, \\forall\\ \\mathbf{\\delta} \\neq 0 \\label{eq:9}\\tag{9}\n",
    "\\end{equation}\n",
    "which reduces to the following:\n",
    "\\begin{equation}\n",
    "    \\nabla f(\\mathbf{x}^*) = 0 \\label{eq:10}\\tag{10}\n",
    "\\end{equation}\n",
    "and\n",
    "\\begin{equation}\n",
    "    \\mathbf{\\delta}\\mathbf{H}f(\\mathbf{x}^*) \\geq 0 \\label{eq:11}\\tag{11}\n",
    "\\end{equation}\n",
    "where equation Eq.\\eqref{eq:11} means that the Hessian matrix should be positive semi-definite.\n",
    "\n",
    "If you are interested in the proofs, refer to the books mentioned or the blog.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Example\n",
    "\n",
    "Let us now work with a new test function called Himmelblau's function, given by,\n",
    "\\begin{equation}\n",
    "    f(\\mathbf{x}) = (x_1^2+x_2-11)^2+(x_1+x_2^2-7)^2 \\label{eq:12}\\tag{12}\n",
    "\\end{equation}\n",
    "where, $\\mathbf{x} \\in \\mathbb{R}^2$. We will check whether $\\mathbf{x}^*=\\begin{bmatrix} 3 \\\\ 2 \\end{bmatrix}$ satisfies the second-order sufficient conditions satisfying the fact that it is a strong local minimizer. We will again use the `autograd` package to do the analyses for this objective function. Let us first define the function and the local minimizer as `x_star` in Python:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Himm(x):\n",
    "    return (x[0]**2 + x[1] - 11)**2 + (x[0] + x[1]**2 - 7)**2\n",
    "x_star = np.array([3, 2], dtype='float') #local minimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then check whether `x_star` is a minimizer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function at x_star: 0.0\n"
     ]
    }
   ],
   "source": [
    "print(\"function at x_star:\", Himm(x_star))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we calculate the gradient vector and the Hessian matrix of the function at `x_star` and look at the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient vector at x_star: [0. 0.]\n",
      "The eigenvalues of M: [82.28427125 25.71572875]\n",
      "M is positive definite\n"
     ]
    }
   ],
   "source": [
    "# import the necessary packages\n",
    "import autograd.numpy as au\n",
    "from autograd import grad, jacobian\n",
    "# gradient vector of the Himmelblau's function\n",
    "Himm_grad=grad(Himm)\n",
    "print(\"gradient vector at x_star:\", Himm_grad(x_star))\n",
    "# Hessian matrix of the Himmelblau's function\n",
    "Himm_hess = jacobian(Himm_grad)\n",
    "M = Himm_hess(x_star) \n",
    "eigs = np.linalg.eigvals(M)\n",
    "print(\"The eigenvalues of M:\", eigs)\n",
    "if (np.all(eigs>0)):\n",
    "    print(\"M is positive definite\")\n",
    "elif (np.all(eigs>=0)):\n",
    "    print(\"M is positive semi-definite\")\n",
    "else:\n",
    "    print(\"M is negative definite\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that `x1` satisfies the second order sufficient conditions and is a strong local minimizer. We wanted to perform the analyses using `autograd` package instead of `scipy.optimize`, because there might be cases when we need to use test functions that are not predefined in `scipy.optimize` package, unlike the `Rosenbrock function`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithms for Solving Unconstrained Minimization Tasks\n",
    "\n",
    "An optimization algorithm for solving an unconstrained minimization problem requires an initial point $\\mathbf{x}_0$ to start with. The choice of $\\mathbf{x}_0$ depends either on the applicant who has some idea about the data and the task at hand or it can be set by the algorithm in charge. Starting with $\\mathbf{x}_0$, the optimization algorithm iterates through a sequence of successive points $\\{\\mathbf{x}_0, \\mathbf{x}_1, \\ldots, \\mathbf{x}_{\\infty}\\}$, which stops at the point where all the termination conditions are met for approximating the minimizer $\\mathbf{x}^*$. The algorithm generates this sequence taking into consideration the objective function $f(\\mathbf{x})$ at a particular point $f(\\mathbf{x}_n)$. A new iterate $\\mathbf{x}_{n+1}$ is added in the sequence if the condition $f(\\mathbf{x}_{n+1}) < f(\\mathbf{x}_n)$. Although in many special cases, the algorithm might fail to find a new point in each and every step following the above condition, it must satisfy that after some stipulated number $k$ of steps, the following condition is met: $$f(\\mathbf{x}_{n+k}) < f(\\mathbf{x}_n)$$. One of the important terminating conditions, for example, is to check whether the first order necessary condition is sufficiently accurate, for a smooth objective function, i.e, $\\|\\nabla f(\\mathbf{x}_{\\infty})\\| < \\epsilon$, where $\\epsilon$ is the infinitesimal tolerance value. We will discuss these conditions further in the subsequent chapters.\n",
    "\n",
    "Fundamentally, there are two approaches available to generate $f(\\mathbf{x}_{n+1})$ from $f(\\mathbf{x}_n)$:\n",
    "\n",
    "* **Line Search Descent Method**: Using this method, the optimization algorithm first picks a direction $\\mathbf{\\delta}_n$ for the $n^{th}$ step and performs a search along this direction from the previous generated iterate $\\mathbf{x}_{n-1}$ to find a new iterate $\\mathbf{x}_n$ such that the condition $f(\\mathbf{x}_n) < f(\\mathbf{x}_{n-1})$ is satisfied. A direction $\\mathbf{\\delta}_n$ is selected for the next iterate if the following condition is satisfied:\n",
    "\n",
    "    \\begin{equation}\n",
    "        \\nabla^T f(\\mathbf{x}_{n-1})\\mathbf{\\delta}_n < 0 \\label{eq:13}\\tag{13}\n",
    "    \\end{equation}\n",
    "    \n",
    "    i.e, if the directional derivative in the direction $\\mathbf{\\delta}_n$ is negative. Here $f$ is the objective function. In view of that, the algorithm then needs to ascertain a distance by which it has to move along the direction $\\mathbf{\\delta}_n$ to figure out $\\mathbf{x}_n$. The distance $\\beta >0$, which is called the *step length*, can be figured out by solving the one-dimensional minimization problem formulated as:\n",
    "    \n",
    "    \\begin{equation}\n",
    "        \\underset{\\beta > 0}{\\min} \\tilde{f}(\\beta) = \\underset{\\beta > 0}{\\min} f(\\mathbf{x}_{n-1} + \\beta \\mathbf{\\delta}_n) \\label{eq:14}\\tag{14} \n",
    "    \\end{equation}\n",
    "\n",
    "\n",
    "* **Trust Region Method**: Using this method, the optimization algorithm develops a model function [refer to Nocedal & Wright], $M_n$, such that its behavior inside a boundary set around the current iterate $\\mathbf{x}_n$ matches that of the objective function $f(\\mathbf{x}_n)$ at that point. The model function is not expected to give a reasonable approximation to the behavior of the objective function at a point $\\mathbf{x}_t$ which is far away from $\\mathbf{x}_n$, i.e, not lying inside the boundary defined around $\\mathbf{x}_n$. As a result, the algorithm obstructs the search for the minimizer of $M_n$ inside the boundary region, which is actually called the *trust region*, denoted by $\\mathcal{T}$, before finding the step $\\mathbf{\\zeta}$, by solving the minimization problem formulated by:\n",
    "\n",
    "    \\begin{equation}\n",
    "        \\underset{\\mathbf{\\zeta}}{\\min\\ } M_n(\\mathbf{x}_n+\\mathbf{\\zeta}),\\text{ where }\\mathbf{x}_n+\\mathbf{\\zeta}\\in \\mathcal{T} \\label{eq:15}\\tag{15}\n",
    "    \\end{equation}\n",
    "    Using this $\\mathbf{\\zeta}$, if the decrease in the value of $f(\\mathbf{x}_{n+1})$ from $f(\\mathbf{x}_n)$ is not sufficient, it can be inferred that the selected trust region is unnecessarily large. The algorithm then reduces the size of $\\mathcal{T}$ accordingly and re-solves the problem given by equation Eq.\\@ref(eq:44). Most often, the trust region $\\mathcal{T}$ is defined by a circle in case of a two dimensional problem or a sphere in case of a three dimensional problem of radius $\\mathcal{T_r}>0$, which follows the condition $\\|\\mathbf{\\zeta}\\| \\leq \\mathcal{T_r}$. In special cases, the shape of the trust region might vary. The form of the model function is given by a quadratic function, given by,\n",
    "\\begin{equation}\n",
    "        M_n(\\mathbf{x}_n+\\mathbf{\\zeta}) = f(\\mathbf{x}_n) + \\mathbf{\\zeta}^T\\nabla f(\\mathbf{x}_n)+\\frac{1}{2}\\mathbf{\\zeta}^T\\mathbf{B} f(\\mathbf{x}_n)\\mathbf{\\zeta} \\label{eq:16}\\tag{16}\n",
    "    \\end{equation}\n",
    "    Where, $\\mathbf{B}f(\\mathbf{x}_n)$ is either the Hessian matrix $\\mathbf{H}f(\\mathbf{x}_n)$ or an approximation to it.\n",
    "\n",
    "Before moving into detailed discussions on line search descent methods and trust region methods in the later chapters, we will first deal with solving equation Eq.\\eqref{eq:14} in the immediate next chapter, which is itself an unconstrained one dimensional minimization problem, where we have to solve for $$\\underset{\\beta>0}{\\min\\ }\\tilde{f}(\\beta)$$ and deduce the value of $\\beta^*$, which is the minimizer for $\\tilde{f}(\\beta)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
